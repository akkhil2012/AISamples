
# saas_ai_dashboard.py - Complete SaaS AI Dashboard with FastAPI Backend and Semantic Cache

import streamlit as st
import streamlit.components.v1 as components
import fastapi
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import threading
import time
import json
import hashlib
import requests
import numpy as np
from typing import Dict, List, Optional, Any
from datetime import datetime
import logging
from contextlib import asynccontextmanager
import asyncio
import os

# Mock LLM responses for demonstration (replace with actual LLM integration)
MOCK_LLM_RESPONSES = {
    "default": "This is a mock LLM response. In a real implementation, this would be generated by an actual language model like GPT, Claude, or Llama.",
    "productivity": "Based on your query about productivity tools, I recommend focusing on workflow automation, task management, and team collaboration features.",
    "communication": "For communication services, consider real-time messaging, video conferencing capabilities, and integration with other business tools.",
    "development": "Development tools should prioritize version control, CI/CD pipelines, project management, and code collaboration features.",
    "marketing": "Marketing platforms excel at customer relationship management, email campaigns, analytics tracking, and lead generation.",
    "analytics": "Analytics services provide insights through data visualization, user behavior tracking, performance metrics, and predictive analysis.",
    "finance": "Financial tools focus on payment processing, accounting automation, expense tracking, and financial reporting.",
    "storage": "Cloud storage solutions offer scalability, security, backup capabilities, and seamless file sharing across teams.",
}

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global data storage
SERVICES_DATA = {
    "nodes": [
        {"id": "office365", "name": "Microsoft 365", "category": "productivity", "size": 30, "price": "$6/user/month"},
        {"id": "gsuite", "name": "Google Workspace", "category": "productivity", "size": 28, "price": "$6/user/month"},
        {"id": "notion", "name": "Notion", "category": "productivity", "size": 22, "price": "$8/user/month"},
        {"id": "slack", "name": "Slack", "category": "communication", "size": 25, "price": "$7.25/user/month"},
        {"id": "teams", "name": "Microsoft Teams", "category": "communication", "size": 24, "price": "$4/user/month"},
        {"id": "github", "name": "GitHub", "category": "development", "size": 26, "price": "$4/user/month"},
        {"id": "gitlab", "name": "GitLab", "category": "development", "size": 21, "price": "$19/user/month"},
        {"id": "hubspot", "name": "HubSpot", "category": "marketing", "size": 24, "price": "$50/month"},
        {"id": "salesforce", "name": "Salesforce", "category": "marketing", "size": 27, "price": "$25/user/month"},
        {"id": "analytics", "name": "Google Analytics", "category": "analytics", "size": 23, "price": "Free / $150K/year"},
        {"id": "stripe", "name": "Stripe", "category": "finance", "size": 24, "price": "2.9% + 30¬¢/transaction"},
        {"id": "aws", "name": "Amazon AWS", "category": "storage", "size": 29, "price": "Pay-as-you-go"},
    ],
    "links": [
        {"source": "office365", "target": "teams", "strength": 3},
        {"source": "gsuite", "target": "slack", "strength": 2},
        {"source": "github", "target": "gitlab", "strength": 2},
        {"source": "hubspot", "target": "salesforce", "strength": 2},
        {"source": "slack", "target": "github", "strength": 2},
        {"source": "teams", "target": "office365", "strength": 3},
        {"source": "stripe", "target": "salesforce", "strength": 2},
        {"source": "aws", "target": "github", "strength": 2},
    ]
}

# Semantic Cache Implementation
class SemanticCache:
    def __init__(self, similarity_threshold: float = 0.8):
        self.cache = {}
        self.similarity_threshold = similarity_threshold
        self.stats = {"hits": 0, "misses": 0}

    def _simple_embedding(self, text: str) -> List[float]:
        """Simple embedding using character frequency (mock implementation)"""
        # In a real implementation, use sentence-transformers or OpenAI embeddings
        chars = "abcdefghijklmnopqrstuvwxyz "
        embedding = [text.lower().count(c) for c in chars]
        # Normalize
        norm = sum(x**2 for x in embedding) ** 0.5
        return [x/norm if norm > 0 else 0 for x in embedding]

    def _cosine_similarity(self, emb1: List[float], emb2: List[float]) -> float:
        """Calculate cosine similarity between embeddings"""
        dot_product = sum(a * b for a, b in zip(emb1, emb2))
        return dot_product

    def get(self, service_id: str, prompt: str) -> Optional[Dict]:
        """Get cached response if exists and similar"""
        prompt_embedding = self._simple_embedding(prompt)

        for cache_key, cache_data in self.cache.items():
            if cache_key.startswith(f"{service_id}:"):
                cached_embedding = cache_data["embedding"]
                similarity = self._cosine_similarity(prompt_embedding, cached_embedding)

                if similarity >= self.similarity_threshold:
                    self.stats["hits"] += 1
                    cache_data["last_accessed"] = datetime.now().isoformat()
                    cache_data["access_count"] = cache_data.get("access_count", 0) + 1
                    logger.info(f"Cache hit for {service_id} with similarity {similarity:.3f}")
                    return cache_data

        self.stats["misses"] += 1
        return None

    def set(self, service_id: str, prompt: str, response: str):
        """Store response in cache"""
        prompt_embedding = self._simple_embedding(prompt)
        cache_key = f"{service_id}:{hashlib.md5(prompt.encode()).hexdigest()[:8]}"

        self.cache[cache_key] = {
            "service_id": service_id,
            "prompt": prompt,
            "response": response,
            "embedding": prompt_embedding,
            "timestamp": datetime.now().isoformat(),
            "access_count": 0
        }
        logger.info(f"Cached response for {service_id}")

    def get_stats(self) -> Dict:
        """Get cache statistics"""
        total_requests = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / total_requests if total_requests > 0 else 0

        return {
            "cache_size": len(self.cache),
            "hits": self.stats["hits"],
            "misses": self.stats["misses"],
            "hit_rate": hit_rate,
            "total_requests": total_requests
        }

# Initialize semantic cache
semantic_cache = SemanticCache()

# FastAPI Models
class QueryRequest(BaseModel):
    service_id: str
    prompt: str

class QueryResponse(BaseModel):
    service_id: str
    prompt: str
    response: str
    cached: bool
    timestamp: str
    processing_time_ms: int

class AddNodeRequest(BaseModel):
    id: str
    name: str
    category: str
    size: int = 20
    price: str = "Contact for pricing"

class AddNodeResponse(BaseModel):
    success: bool
    message: str
    node: Dict

# Mock LLM Integration
def call_llm(service_id: str, prompt: str, category: str) -> str:
    """
    Mock LLM integration. In production, replace with actual LLM calls:
    - OpenAI GPT API
    - Anthropic Claude API  
    - Local Llama/Mistral models
    - Azure OpenAI Service
    """
    # Simulate processing time
    time.sleep(0.5)  # Mock API call delay

    # Generate contextual response based on service category
    base_response = MOCK_LLM_RESPONSES.get(category, MOCK_LLM_RESPONSES["default"])

    contextualized_response = f"""
Based on your query about {service_id}: "{prompt}"

{base_response}

Specific recommendations for {service_id}:
‚Ä¢ Consider integration capabilities with your existing tech stack
‚Ä¢ Evaluate pricing models that align with your usage patterns  
‚Ä¢ Review security and compliance features for your industry
‚Ä¢ Assess scalability options for future growth

This response was generated by the AI system at {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}.
"""

    return contextualized_response.strip()

# FastAPI Application
app = FastAPI(title="SaaS AI Dashboard API", version="1.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "SaaS AI Dashboard API", "status": "running"}

@app.get("/health")
async def health():
    cache_stats = semantic_cache.get_stats()
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": len(SERVICES_DATA["nodes"]),
        "cache_stats": cache_stats
    }

@app.post("/api/query", response_model=QueryResponse)
async def process_query(request: QueryRequest):
    """Process user query with semantic caching and LLM integration"""
    start_time = time.time()

    try:
        # Check semantic cache first
        cached_result = semantic_cache.get(request.service_id, request.prompt)

        if cached_result:
            # Cache hit
            processing_time = int((time.time() - start_time) * 1000)
            return QueryResponse(
                service_id=request.service_id,
                prompt=request.prompt,
                response=cached_result["response"],
                cached=True,
                timestamp=datetime.now().isoformat(),
                processing_time_ms=processing_time
            )

        # Cache miss - call LLM
        # Find service category for context
        service_category = "default"
        for node in SERVICES_DATA["nodes"]:
            if node["id"] == request.service_id:
                service_category = node["category"]
                break

        # Generate response using LLM
        llm_response = call_llm(request.service_id, request.prompt, service_category)

        # Store in cache
        semantic_cache.set(request.service_id, request.prompt, llm_response)

        processing_time = int((time.time() - start_time) * 1000)

        return QueryResponse(
            service_id=request.service_id,
            prompt=request.prompt,
            response=llm_response,
            cached=False,
            timestamp=datetime.now().isoformat(),
            processing_time_ms=processing_time
        )

    except Exception as e:
        logger.error(f"Error processing query: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/services")
async def get_services():
    """Get all services data"""
    return SERVICES_DATA

@app.post("/api/services", response_model=AddNodeResponse)
async def add_service(request: AddNodeRequest):
    """Add a new service node"""
    try:
        # Check if service already exists
        existing_ids = [node["id"] for node in SERVICES_DATA["nodes"]]
        if request.id in existing_ids:
            return AddNodeResponse(
                success=False,
                message=f"Service with ID '{request.id}' already exists",
                node={}
            )

        # Create new node
        new_node = {
            "id": request.id,
            "name": request.name,
            "category": request.category,
            "size": request.size,
            "price": request.price
        }

        # Add to services data
        SERVICES_DATA["nodes"].append(new_node)

        logger.info(f"Added new service: {request.name}")

        return AddNodeResponse(
            success=True,
            message=f"Successfully added service '{request.name}'",
            node=new_node
        )

    except Exception as e:
        logger.error(f"Error adding service: {e}")
        return AddNodeResponse(
            success=False,
            message=f"Error adding service: {str(e)}",
            node={}
        )

@app.get("/api/cache/stats")
async def get_cache_stats():
    """Get semantic cache statistics"""
    return semantic_cache.get_stats()

# Function to run FastAPI server
def run_fastapi():
    """Run FastAPI server in a separate thread"""
    uvicorn.run(app, host="127.0.0.1", port=8000, log_level="info")

# Streamlit Application
def create_d3_graph_html():
    """Create the enhanced D3.js graph with modal functionality"""

    services_json = json.dumps(SERVICES_DATA)

    html_code = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            body {{
                margin: 0;
                padding: 20px;
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                min-height: 100vh;
            }}

            .container {{
                max-width: 1200px;
                margin: 0 auto;
                background: white;
                border-radius: 15px;
                padding: 20px;
                box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            }}

            .header {{
                text-align: center;
                margin-bottom: 20px;
                padding-bottom: 15px;
                border-bottom: 2px solid #e9ecef;
            }}

            .controls {{
                text-align: center;
                margin-bottom: 15px;
            }}

            .btn {{
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                border: none;
                padding: 10px 20px;
                margin: 5px;
                border-radius: 25px;
                cursor: pointer;
                font-size: 14px;
                transition: all 0.3s ease;
            }}

            .btn:hover {{
                transform: translateY(-2px);
                box-shadow: 0 6px 20px rgba(0,0,0,0.3);
            }}

            .graph-area {{
                border: 2px solid #e9ecef;
                border-radius: 10px;
                background: #ffffff;
                position: relative;
            }}

            .node {{
                cursor: pointer;
                transition: all 0.3s ease;
            }}

            .node:hover {{
                stroke: #333;
                stroke-width: 3px;
                filter: brightness(1.2);
            }}

            .link {{
                stroke: #999;
                stroke-opacity: 0.6;
            }}

            .node-label {{
                font-family: 'Segoe UI', sans-serif;
                font-size: 11px;
                font-weight: 500;
                fill: #2c3e50;
                text-anchor: middle;
                pointer-events: none;
            }}

            /* Modal Styles */
            .modal {{
                display: none;
                position: fixed;
                z-index: 1000;
                left: 0;
                top: 0;
                width: 100%;
                height: 100%;
                background-color: rgba(0,0,0,0.7);
                backdrop-filter: blur(5px);
            }}

            .modal-content {{
                background-color: white;
                margin: 5% auto;
                padding: 0;
                border-radius: 15px;
                width: 90%;
                max-width: 600px;
                box-shadow: 0 20px 60px rgba(0,0,0,0.3);
                animation: modalSlideIn 0.3s ease;
            }}

            @keyframes modalSlideIn {{
                from {{ opacity: 0; transform: translateY(-50px); }}
                to {{ opacity: 1; transform: translateY(0); }}
            }}

            .modal-header {{
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 20px;
                border-radius: 15px 15px 0 0;
                position: relative;
            }}

            .modal-body {{
                padding: 30px;
            }}

            .modal-footer {{
                padding: 20px 30px;
                border-top: 1px solid #eee;
                text-align: right;
            }}

            .close {{
                position: absolute;
                right: 20px;
                top: 15px;
                color: white;
                font-size: 28px;
                font-weight: bold;
                cursor: pointer;
                width: 35px;
                height: 35px;
                display: flex;
                align-items: center;
                justify-content: center;
                border-radius: 50%;
                transition: background-color 0.3s;
            }}

            .close:hover {{
                background-color: rgba(255,255,255,0.2);
            }}

            .form-group {{
                margin-bottom: 20px;
            }}

            .form-label {{
                display: block;
                margin-bottom: 8px;
                font-weight: 600;
                color: #333;
            }}

            .form-control {{
                width: 100%;
                padding: 12px;
                border: 2px solid #e1e5e9;
                border-radius: 8px;
                font-size: 14px;
                font-family: inherit;
                transition: border-color 0.3s;
                box-sizing: border-box;
            }}

            .form-control:focus {{
                outline: none;
                border-color: #667eea;
                box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);
            }}

            textarea.form-control {{
                resize: vertical;
                min-height: 120px;
            }}

            .btn-primary {{
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                border: none;
                padding: 12px 30px;
                border-radius: 8px;
                color: white;
                font-weight: 600;
                cursor: pointer;
                transition: all 0.3s ease;
            }}

            .btn-primary:hover {{
                transform: translateY(-1px);
                box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
            }}

            .btn-primary:disabled {{
                background: #ccc;
                cursor: not-allowed;
                transform: none;
                box-shadow: none;
            }}

            .response-area {{
                background: #f8f9fa;
                border: 1px solid #e9ecef;
                border-radius: 8px;
                padding: 20px;
                margin-top: 20px;
                display: none;
            }}

            .response-header {{
                display: flex;
                justify-content: space-between;
                align-items: center;
                margin-bottom: 15px;
                padding-bottom: 10px;
                border-bottom: 1px solid #dee2e6;
            }}

            .cache-badge {{
                padding: 4px 12px;
                border-radius: 20px;
                font-size: 12px;
                font-weight: 600;
                text-transform: uppercase;
            }}

            .cache-hit {{
                background: #d4edda;
                color: #155724;
            }}

            .cache-miss {{
                background: #f8d7da;
                color: #721c24;
            }}

            .response-text {{
                line-height: 1.6;
                color: #333;
                white-space: pre-wrap;
            }}

            .loading {{
                text-align: center;
                padding: 20px;
                color: #666;
            }}

            .spinner {{
                border: 3px solid #f3f3f3;
                border-top: 3px solid #667eea;
                border-radius: 50%;
                width: 30px;
                height: 30px;
                animation: spin 1s linear infinite;
                margin: 0 auto 15px auto;
            }}

            @keyframes spin {{
                0% {{ transform: rotate(0deg); }}
                100% {{ transform: rotate(360deg); }}
            }}
        </style>
        <script src="https://d3js.org/d3.v7.min.js"></script>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>ü§ñ AI-Powered SaaS Network</h1>
                <p>Click on any service node to interact with AI</p>
            </div>

            <div class="controls">
                <button class="btn" onclick="zoomIn()">üîç Zoom In</button>
                <button class="btn" onclick="zoomOut()">üîç Zoom Out</button>
                <button class="btn" onclick="resetZoom()">üîÑ Reset View</button>
                <button class="btn" onclick="toggleLabels()">üè∑Ô∏è Toggle Labels</button>
            </div>

            <div class="graph-area">
                <svg id="graph"></svg>
            </div>
        </div>

        <!-- Modal Dialog -->
        <div id="serviceModal" class="modal">
            <div class="modal-content">
                <div class="modal-header">
                    <h2 id="modalTitle">Service AI Assistant</h2>
                    <span class="close" onclick="closeModal()">&times;</span>
                </div>
                <div class="modal-body">
                    <div class="form-group">
                        <label class="form-label" for="promptInput">Ask anything about this service:</label>
                        <textarea id="promptInput" class="form-control" 
                                placeholder="e.g., How does this service integrate with other tools? What are the pricing options? What features should I consider?"></textarea>
                    </div>
                    <div id="responseArea" class="response-area">
                        <div id="loadingIndicator" class="loading" style="display: none;">
                            <div class="spinner"></div>
                            <p>AI is thinking...</p>
                        </div>
                        <div id="responseContent" style="display: none;">
                            <div class="response-header">
                                <strong>AI Response:</strong>
                                <span id="cacheBadge" class="cache-badge"></span>
                            </div>
                            <div id="responseText" class="response-text"></div>
                        </div>
                    </div>
                </div>
                <div class="modal-footer">
                    <button id="submitBtn" class="btn-primary" onclick="submitQuery()">Ask AI Assistant</button>
                </div>
            </div>
        </div>

        <script>
            // Data and configuration
            const data = {services_json};
            const API_BASE = 'http://localhost:8000';

            // Color scale for categories
            const colorScale = {{
                'productivity': '#ff6b6b',
                'communication': '#4ecdc4', 
                'development': '#45b7d1',
                'marketing': '#f9ca24',
                'analytics': '#6c5ce7',
                'design': '#fd79a8',
                'finance': '#00b894',
                'storage': '#e17055'
            }};

            // Graph setup
            const container = d3.select('.graph-area');
            const containerRect = container.node().getBoundingClientRect();
            const width = containerRect.width;
            const height = 600;

            const svg = d3.select("#graph")
                .attr("width", width)
                .attr("height", height);

            // Zoom behavior
            const zoom = d3.zoom()
                .scaleExtent([0.3, 3])
                .on("zoom", (event) => {{
                    container_g.attr("transform", event.transform);
                }});

            svg.call(zoom);
            const container_g = svg.append("g");

            // Force simulation
            const simulation = d3.forceSimulation(data.nodes)
                .force("link", d3.forceLink(data.links).id(d => d.id).distance(80).strength(0.1))
                .force("charge", d3.forceManyBody().strength(-200))
                .force("center", d3.forceCenter(width / 2, height / 2))
                .force("collision", d3.forceCollide().radius(d => d.size + 5));

            // Create links
            const link = container_g.append("g")
                .selectAll("line")
                .data(data.links)
                .enter().append("line")
                .attr("class", "link")
                .style("stroke-width", 2);

            // Create nodes
            const node = container_g.append("g")
                .selectAll("circle")
                .data(data.nodes)
                .enter().append("circle")
                .attr("class", "node")
                .attr("r", d => d.size)
                .attr("fill", d => colorScale[d.category] || '#999')
                .attr("stroke", "#fff")
                .attr("stroke-width", 2)
                .call(d3.drag()
                    .on("start", dragstarted)
                    .on("drag", dragged)
                    .on("end", dragended))
                .on("click", openModal);

            // Create labels
            const labels = container_g.append("g")
                .selectAll("text")
                .data(data.nodes)
                .enter().append("text")
                .attr("class", "node-label")
                .text(d => d.name)
                .attr("dy", d => d.size + 15);

            // Animation loop
            simulation.on("tick", () => {{
                link
                    .attr("x1", d => d.source.x)
                    .attr("y1", d => d.source.y)
                    .attr("x2", d => d.target.x)
                    .attr("y2", d => d.target.y);

                node
                    .attr("cx", d => d.x)
                    .attr("cy", d => d.y);

                labels
                    .attr("x", d => d.x)
                    .attr("y", d => d.y);
            }});

            // Drag functions
            function dragstarted(event, d) {{
                if (!event.active) simulation.alphaTarget(0.3).restart();
                d.fx = d.x;
                d.fy = d.y;
            }}

            function dragged(event, d) {{
                d.fx = event.x;
                d.fy = event.y;
            }}

            function dragended(event, d) {{
                if (!event.active) simulation.alphaTarget(0);
                d.fx = null;
                d.fy = null;
            }}

            // Modal functions
            let currentService = null;

            function openModal(event, d) {{
                currentService = d;
                document.getElementById('modalTitle').textContent = `${d.name} - AI Assistant`;
                document.getElementById('promptInput').value = '';
                document.getElementById('responseArea').style.display = 'none';
                document.getElementById('serviceModal').style.display = 'block';
                document.getElementById('promptInput').focus();
            }}

            function closeModal() {{
                document.getElementById('serviceModal').style.display = 'none';
                currentService = null;
            }}

            // Handle Enter key in textarea
            document.getElementById('promptInput').addEventListener('keydown', function(event) {{
                if (event.key === 'Enter' && event.ctrlKey) {{
                    submitQuery();
                }}
            }});

            async function submitQuery() {{
                const prompt = document.getElementById('promptInput').value.trim();
                if (!prompt) {{
                    alert('Please enter a question or prompt');
                    return;
                }}

                const submitBtn = document.getElementById('submitBtn');
                const responseArea = document.getElementById('responseArea');
                const loadingIndicator = document.getElementById('loadingIndicator');
                const responseContent = document.getElementById('responseContent');

                // Show loading state
                submitBtn.disabled = true;
                submitBtn.textContent = 'Processing...';
                responseArea.style.display = 'block';
                loadingIndicator.style.display = 'block';
                responseContent.style.display = 'none';

                try {{
                    const response = await fetch(`${{API_BASE}}/api/query`, {{
                        method: 'POST',
                        headers: {{
                            'Content-Type': 'application/json',
                        }},
                        body: JSON.stringify({{
                            service_id: currentService.id,
                            prompt: prompt
                        }})
                    }});

                    if (!response.ok) {{
                        throw new Error(`HTTP error! status: ${{response.status}}`);
                    }}

                    const result = await response.json();

                    // Display response
                    loadingIndicator.style.display = 'none';
                    responseContent.style.display = 'block';

                    const cacheBadge = document.getElementById('cacheBadge');
                    const responseText = document.getElementById('responseText');

                    cacheBadge.textContent = result.cached ? 'Cached' : 'Fresh';
                    cacheBadge.className = `cache-badge ${{result.cached ? 'cache-hit' : 'cache-miss'}}`;
                    responseText.textContent = result.response;

                }} catch (error) {{
                    console.error('Error:', error);
                    loadingIndicator.style.display = 'none';
                    responseContent.style.display = 'block';
                    document.getElementById('responseText').textContent = 
                        'Sorry, there was an error processing your request. Please make sure the FastAPI backend is running on port 8000.';
                    document.getElementById('cacheBadge').textContent = 'Error';
                    document.getElementById('cacheBadge').className = 'cache-badge cache-miss';
                }} finally {{
                    submitBtn.disabled = false;
                    submitBtn.textContent = 'Ask AI Assistant';
                }}
            }}

            // Control functions
            let labelsVisible = true;

            function zoomIn() {{
                svg.transition().duration(300).call(zoom.scaleBy, 1.5);
            }}

            function zoomOut() {{
                svg.transition().duration(300).call(zoom.scaleBy, 1 / 1.5);
            }}

            function resetZoom() {{
                svg.transition().duration(500).call(zoom.transform, d3.zoomIdentity);
            }}

            function toggleLabels() {{
                labelsVisible = !labelsVisible;
                labels.style("opacity", labelsVisible ? 1 : 0);
            }}

            // Close modal when clicking outside
            window.onclick = function(event) {{
                const modal = document.getElementById('serviceModal');
                if (event.target === modal) {{
                    closeModal();
                }}
            }}

            // Initial entrance animation
            setTimeout(() => {{
                node
                    .attr("r", 0)
                    .transition()
                    .duration(1000)
                    .delay((d, i) => i * 100)
                    .attr("r", d => d.size);
            }}, 100);
        </script>
    </body>
    </html>
    """

    return html_code

def run_streamlit_app():
    """Main Streamlit application"""

    # Page configuration
    st.set_page_config(
        page_title="AI SaaS Dashboard",
        page_icon="ü§ñ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # Custom CSS
    st.markdown("""
    <style>
        .main-header {
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            padding: 2rem;
            border-radius: 15px;
            margin-bottom: 2rem;
            color: white;
            text-align: center;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }

        .stats-card {
            background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%);
            padding: 1.5rem;
            border-radius: 10px;
            color: white;
            text-align: center;
            margin: 1rem 0;
        }

        .add-service-form {
            background: white;
            padding: 1.5rem;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin: 1rem 0;
            border-left: 4px solid #667eea;
        }
    </style>
    """, unsafe_allow_html=True)

    # Header
    st.markdown("""
    <div class="main-header">
        <h1>ü§ñ AI-Powered SaaS Dashboard</h1>
        <p>Interactive network with semantic caching and LLM integration</p>
    </div>
    """, unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.header("üéõÔ∏è Dashboard Controls")

        # Backend status check
        try:
            response = requests.get("http://localhost:8000/health", timeout=2)
            if response.status_code == 200:
                data = response.json()
                st.success("‚úÖ Backend API Connected")

                # Cache statistics
                st.subheader("üìä Cache Statistics")
                cache_stats = data.get("cache_stats", {})
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Cache Size", cache_stats.get("cache_size", 0))
                    st.metric("Cache Hits", cache_stats.get("hits", 0))
                with col2:
                    hit_rate = cache_stats.get("hit_rate", 0)
                    st.metric("Hit Rate", f"{hit_rate:.1%}")
                    st.metric("Total Queries", cache_stats.get("total_requests", 0))

            else:
                st.error("‚ùå Backend API Error")

        except requests.exceptions.RequestException:
            st.warning("‚ö†Ô∏è Backend API Disconnected")
            st.info("Make sure FastAPI server is running on port 8000")

        # Add new service
        st.header("‚ûï Add New Service")

        with st.form("add_service_form"):
            new_id = st.text_input("Service ID", placeholder="e.g., new-service")
            new_name = st.text_input("Service Name", placeholder="e.g., New Service")
            new_category = st.selectbox("Category", [
                "productivity", "communication", "development", 
                "marketing", "analytics", "design", "finance", "storage"
            ])
            new_size = st.slider("Node Size", 15, 35, 20)
            new_price = st.text_input("Pricing", placeholder="e.g., $10/month")

            submitted = st.form_submit_button("Add Service")

            if submitted and new_id and new_name:
                try:
                    response = requests.post("http://localhost:8000/api/services", 
                        json={
                            "id": new_id,
                            "name": new_name,
                            "category": new_category,
                            "size": new_size,
                            "price": new_price
                        }, 
                        timeout=5
                    )

                    if response.status_code == 200:
                        result = response.json()
                        if result["success"]:
                            st.success(f"‚úÖ {result['message']}")
                            st.info("Refresh the page to see the new service in the graph")
                        else:
                            st.error(f"‚ùå {result['message']}")
                    else:
                        st.error("Failed to add service")

                except requests.exceptions.RequestException:
                    st.error("Cannot connect to backend API")

        # Instructions
        st.header("üìñ How to Use")
        st.markdown("""
        **Graph Interactions:**
        - **Click nodes** to open AI assistant modal
        - **Drag nodes** to reposition them  
        - **Zoom/Pan** to explore the network
        - **Toggle labels** to show/hide names

        **AI Assistant:**
        - Ask questions about any service
        - Get contextual AI responses
        - Responses are cached for efficiency
        - Try asking about integrations, pricing, features

        **Adding Services:**
        - Use the form above to add new nodes
        - Services appear immediately in the network
        - Choose appropriate category and size
        """)

    # Main content
    col1, col2 = st.columns([3, 1])

    with col1:
        st.header("üåê Interactive AI Network")

        # Embed the D3.js graph
        components.html(create_d3_graph_html(), height=700)

        st.markdown("""
        **üí° Try asking the AI:**
        - "How does this service integrate with other tools?"
        - "What are the main features and benefits?"  
        - "Compare pricing models and recommendations"
        - "What should I consider before implementing this?"
        """)

    with col2:
        st.header("üìà System Info")

        # System statistics
        st.markdown("""
        <div class="stats-card">
            <h3>üéØ Features</h3>
            <p><strong>‚úÖ D3.js Force Graph</strong><br/>
            ‚úÖ Modal Windows<br/>
            ‚úÖ Semantic Caching<br/>
            ‚úÖ LLM Integration<br/>
            ‚úÖ Add New Nodes<br/>
            ‚úÖ Zoom & Pan Controls</p>
        </div>
        """, unsafe_allow_html=True)

        # Current services
        st.subheader("üîß Current Services")
        total_services = len(SERVICES_DATA["nodes"])
        st.metric("Total Services", total_services)

        # Service categories
        categories = {}
        for node in SERVICES_DATA["nodes"]:
            cat = node["category"]
            categories[cat] = categories.get(cat, 0) + 1

        for category, count in sorted(categories.items()):
            st.write(f"**{category.title()}:** {count}")

        # Recent additions
        if st.button("üîÑ Refresh Data"):
            st.rerun()

def main():
    """Main function to run the complete application"""

    # Check if FastAPI is already running
    try:
        response = requests.get("http://localhost:8000/health", timeout=1)
        if response.status_code == 200:
            st.info("‚ÑπÔ∏è FastAPI backend is already running")
        else:
            st.error("‚ùå FastAPI backend responded with an error")
    except requests.exceptions.RequestException:
        # Start FastAPI in a separate thread
        st.info("üöÄ Starting FastAPI backend...")
        fastapi_thread = threading.Thread(target=run_fastapi, daemon=True)
        fastapi_thread.start()

        # Wait a moment for the server to start
        time.sleep(2)

        # Verify it started
        try:
            response = requests.get("http://localhost:8000/health", timeout=5)
            if response.status_code == 200:
                st.success("‚úÖ FastAPI backend started successfully!")
            else:
                st.error("‚ùå FastAPI backend failed to start properly")
        except requests.exceptions.RequestException:
            st.error("‚ùå Could not connect to FastAPI backend")

    # Run the Streamlit app
    run_streamlit_app()

if __name__ == "__main__":
    main()
